#!/usr/bin/env python3
"""
çœŸå®arXivæœç´¢è„šæœ¬
"""

import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
import time

def search_arxiv(keywords, days=7, max_results=10):
    """æœç´¢arXivæ–‡çŒ®"""
    
    # arXiv APIåŸºç¡€URL
    base_url = "http://export.arxiv.org/api/query"
    
    # æ„å»ºæŸ¥è¯¢ - æœ€è¿‘Nå¤©çš„æ–‡çŒ®
    date_cutoff = (datetime.now() - timedelta(days=days)).strftime('%Y%m%d')
    search_query = f'({keywords}) AND submittedDate:[{date_cutoff}000000 TO *]'
    
    # æ„å»ºæŸ¥è¯¢å‚æ•°
    params = {
        'search_query': search_query,
        'start': 0,
        'max_results': max_results,
        'sortBy': 'submittedDate',
        'sortOrder': 'descending'
    }
    
    # ç¼–ç URL
    query_string = urllib.parse.urlencode(params)
    url = f"{base_url}?{query_string}"
    
    print(f"æœç´¢å…³é”®è¯: {keywords}")
    print(f"æ—¶é—´èŒƒå›´: æœ€è¿‘{days}å¤©")
    print(f"APIè¯·æ±‚: {url}")
    print("-" * 80)
    
    try:
        # å‘é€è¯·æ±‚
        req = urllib.request.Request(url, headers={'User-Agent': 'OpenClaw/1.0'})
        response = urllib.request.urlopen(req, timeout=30)
        xml_data = response.read().decode('utf-8')
        
        # è§£æXML
        root = ET.fromstring(xml_data)
        
        # Atomå‘½åç©ºé—´
        ns = {'atom': 'http://www.w3.org/2005/Atom'}
        
        # è·å–æ€»ç»“æœæ•°
        total_results = root.find('atom:opensearch:totalResults', ns)
        total = int(total_results.text) if total_results is not None else 0
        
        print(f"æ‰¾åˆ°æ–‡çŒ®: {total} ç¯‡")
        
        # è§£ææ¡ç›®
        entries = root.findall('atom:entry', ns)
        results = []
        
        for i, entry in enumerate(entries[:max_results]):
            # æå–æ ‡é¢˜
            title_elem = entry.find('atom:title', ns)
            title = title_elem.text.strip() if title_elem is not None else "æ— æ ‡é¢˜"
            
            # æå–ä½œè€…
            authors = []
            for author in entry.findall('atom:author', ns):
                name_elem = author.find('atom:name', ns)
                if name_elem is not None:
                    authors.append(name_elem.text)
            
            # æå–æ‘˜è¦
            summary_elem = entry.find('atom:summary', ns)
            summary = summary_elem.text.strip() if summary_elem is not None else "æ— æ‘˜è¦"
            
            # æå–å‘å¸ƒæ—¶é—´
            published_elem = entry.find('atom:published', ns)
            published = published_elem.text[:10] if published_elem is not None else "æœªçŸ¥"
            
            # æå–arXiv ID
            id_elem = entry.find('atom:id', ns)
            arxiv_id = id_elem.text if id_elem is not None else ""
            
            # æå–PDFé“¾æ¥
            pdf_link = ""
            for link in entry.findall('atom:link', ns):
                if link.get('title') == 'pdf':
                    pdf_link = link.get('href')
                    break
            
            # æå–åˆ†ç±»
            categories = []
            for category in entry.findall('atom:category', ns):
                cat_term = category.get('term', '')
                if cat_term:
                    categories.append(cat_term)
            
            # æ„å»ºç»“æœ
            result = {
                'index': i + 1,
                'title': title,
                'authors': authors,
                'published': published,
                'arxiv_id': arxiv_id,
                'pdf_link': pdf_link,
                'summary': summary,
                'categories': categories[:3]  # åªæ˜¾ç¤ºå‰3ä¸ªåˆ†ç±»
            }
            results.append(result)
        
        return results, total
        
    except Exception as e:
        print(f"APIè¯·æ±‚å¤±è´¥: {e}")
        return [], 0

def format_results(results, keyword):
    """æ ¼å¼åŒ–ç»“æœ"""
    
    if not results:
        return "æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®ã€‚"
    
    output = []
    output.append(f"## ğŸ” å…³é”®è¯: {keyword}")
    output.append("")
    
    for result in results:
        output.append(f"### {result['index']}. {result['title']}")
        output.append("")
        
        # ä½œè€…ä¿¡æ¯
        authors_display = ', '.join(result['authors'][:3])
        if len(result['authors']) > 3:
            authors_display += f" ç­‰ ({len(result['authors'])}ä½ä½œè€…)"
        output.append(f"**ä½œè€…**: {authors_display}")
        
        # å‘å¸ƒæ—¶é—´å’Œåˆ†ç±»
        output.append(f"**å‘å¸ƒæ—¶é—´**: {result['published']}")
        if result['categories']:
            output.append(f"**åˆ†ç±»**: {', '.join(result['categories'])}")
        
        # é“¾æ¥
        if result['arxiv_id']:
            output.append(f"**arXivé“¾æ¥**: {result['arxiv_id']}")
        if result['pdf_link']:
            output.append(f"**PDFä¸‹è½½**: {result['pdf_link']}")
        
        # æ‘˜è¦
        summary_preview = result['summary'][:300] + "..." if len(result['summary']) > 300 else result['summary']
        output.append(f"**æ‘˜è¦**: {summary_preview}")
        
        output.append("")
        output.append("---")
        output.append("")
    
    return '\n'.join(output)

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("arXivçœŸå®æ–‡çŒ®æœç´¢")
    print("=" * 80)
    
    # æœç´¢å…³é”®è¯
    keywords_list = [
        "quantum spin liquid",
        "magnetoelectric coupling", 
        "multiferroic",
        "topological insulator",
        "skyrmion",
        "spintronics"
    ]
    
    all_results = []
    
    for keyword in keywords_list:
        print(f"\næœç´¢: {keyword}")
        print("-" * 40)
        
        results, total = search_arxiv(keyword, days=30, max_results=3)
        
        if results:
            formatted = format_results(results, keyword)
            all_results.append(formatted)
            
            # æ˜¾ç¤ºç®€è¦ä¿¡æ¯
            print(f"æ‰¾åˆ° {len(results)} ç¯‡æ–‡çŒ®:")
            for result in results:
                print(f"  {result['index']}. {result['title'][:60]}...")
        
        # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
        time.sleep(2)
    
    # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
    if all_results:
        print("\n" + "=" * 80)
        print("ç”Ÿæˆå®Œæ•´æŠ¥å‘Š...")
        print("=" * 80)
        
        report = f"""# ğŸ“š arXivæ–‡çŒ®æœç´¢æŠ¥å‘Š

**æŠ¥å‘Šæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**æœç´¢èŒƒå›´**: æœ€è¿‘30å¤©
**æœç´¢å¹³å°**: arXiv.org

---

{'\n\n'.join(all_results)}

---

## ğŸ“Š æœç´¢æ€»ç»“

**æœç´¢å®Œæˆæ—¶é—´**: {datetime.now().strftime('%H:%M:%S')}
**æ•°æ®æ¥æº**: arXivå®˜æ–¹API (http://export.arxiv.org/api/query)
**çŠ¶æ€**: âœ… çœŸå®æ•°æ®è·å–æˆåŠŸ

---

*æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆäº OpenClaw arXivç›‘æ§ç³»ç»Ÿ*
*æ•°æ®æ¥æº: arXiv.org - åº·å¥ˆå°”å¤§å­¦*
"""
        
        # ä¿å­˜æŠ¥å‘Š
        import os
        os.makedirs('./reports', exist_ok=True)
        filename = f"arxiv_real_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        filepath = os.path.join('./reports', filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜: {filepath}")
        print(f"ğŸ“„ æŠ¥å‘Šå¤§å°: {len(report)} å­—ç¬¦")
        
        # æ˜¾ç¤ºæŠ¥å‘Šæ‘˜è¦
        print("\nğŸ“‹ æŠ¥å‘Šæ‘˜è¦:")
        print("-" * 40)
        lines = report.split('\n')
        for i in range(min(20, len(lines))):
            print(lines[i])
        
        return report, filepath
    else:
        print("\nâš ï¸ æœªæ‰¾åˆ°ä»»ä½•æ–‡çŒ®")
        return None, None

if __name__ == "__main__":
    report, filepath = main()