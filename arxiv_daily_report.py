#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
arXivæ¯æ—¥æ–‡çŒ®æŠ¥å‘Šç”Ÿæˆï¼ˆç®€åŒ–é…ç½®ç‰ˆï¼‰
åªéœ€è¦åœ¨ KEYWORDS åˆ—è¡¨é‡Œå¡«å†™å…³é”®è¯ï¼Œè‡ªåŠ¨ç”Ÿæˆ abs å­—æ®µæŸ¥è¯¢
"""

import os
import sys
import re
import time
import argparse
import hashlib
import base64
import hmac
import json
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
from urllib.parse import quote_plus
from pathlib import Path

import requests

# ==================== ç®€åŒ–é…ç½®åŒº ====================
# åœ¨è¿™é‡Œå¡«å†™æ‚¨æ„Ÿå…´è¶£çš„å…³é”®è¯ï¼Œæ¯è¡Œä¸€ä¸ª
KEYWORDS = [
    "quantum spin liquid",
    "frustrated magnet",
    "pyrochlore",
    "kagome",
    "single crystal growth",
    "magnetoelectric",
    "multiferroic",
    "QSL",
    "geometric frustration"
]

# ç»„åˆè¯æœ€å¤§æ•°é‡ï¼ˆè‡ªåŠ¨ç”Ÿæˆä¸¤ä¸¤ç»„åˆï¼Œå¢åŠ è¦†ç›–ï¼‰
MAX_COMBINE = 2

# æ¯ä¸ªä¸»é¢˜çš„ç›®æ ‡è®ºæ–‡æ•°é‡
TARGET_COUNT = 5

# è¾“å‡ºç›®å½•
OUTPUT_DIR = Path("./reports")
OUTPUT_DIR.mkdir(exist_ok=True)

# å·²å‘é€IDè®°å½•
SENT_IDS_FILE = Path("./sent_papers.json")
# ===================================================

def load_sent_ids():
    if SENT_IDS_FILE.exists():
        try:
            return set(json.loads(SENT_IDS_FILE.read_text(encoding='utf-8')))
        except:
            return set()
    return set()

def save_sent_ids(ids):
    SENT_IDS_FILE.write_text(json.dumps(list(ids), indent=2), encoding='utf-8')

def generate_queries(keywords, max_combine=2):
    """
    ä»å…³é”®è¯åˆ—è¡¨ç”ŸæˆæŸ¥è¯¢è¯­å¥åˆ—è¡¨
    ç”Ÿæˆç­–ç•¥ï¼š
    - æ¯ä¸ªå…³é”®è¯å•ç‹¬ä½œä¸º abs:"å…³é”®è¯"
    - æ¯ä¸¤ä¸ªå…³é”®è¯ç»„åˆä¸º abs:"è¯1" abs:"è¯2"
    """
    queries = []
    # å•å…³é”®è¯
    for k in keywords:
        if k.strip():
            queries.append(f'abs:"{k.strip()}"')
    # ä¸¤è¯ç»„åˆ
    if max_combine >= 2:
        for i in range(len(keywords)):
            for j in range(i+1, len(keywords)):
                if keywords[i].strip() and keywords[j].strip():
                    queries.append(f'abs:"{keywords[i].strip()}" abs:"{keywords[j].strip()}"')
    # å»é‡
    return list(set(queries))

def query_arxiv_raw(query_str, max_results=30):
    base_url = "https://export.arxiv.org/api/query"
    url = f"{base_url}?search_query={quote_plus(query_str)}&sortBy=submittedDate&sortOrder=descending&start=0&max_results={max_results}"
    resp = requests.get(url, timeout=30)
    resp.raise_for_status()
    return resp.text

def parse_arxiv_entry(entry_xml):
    ns = {'arxiv': 'http://www.w3.org/2005/Atom'}
    entry = ET.fromstring(entry_xml)
    title = entry.find('arxiv:title', ns).text.strip()
    summary = entry.find('arxiv:summary', ns).text.strip()
    link = entry.find('arxiv:id', ns).text
    paper_id = link.replace('http://arxiv.org/abs/', 'arxiv:')
    published = entry.find('arxiv:published', ns).text
    pub_dt = datetime.strptime(published[:19], "%Y-%m-%dT%H:%M:%S").replace(tzinfo=timezone.utc)
    authors = [author.find('arxiv:name', ns).text for author in entry.findall('arxiv:author', ns)]
    categories = [cat.get('term') for cat in entry.findall('arxiv:category', ns)]
    return {
        'id': paper_id,
        'title': title,
        'summary': summary,
        'link': link,
        'published': published,
        'authors': authors,
        'categories': categories,
        'pub_dt': pub_dt
    }

def fetch_arxiv_papers(queries, since_dt, target_count):
    collected = []
    seen_ids = set()
    for q in queries:
        if len(collected) >= target_count:
            break
        try:
            xml = query_arxiv_raw(q, max_results=25)
            for entry_xml in xml.split('<entry>')[1:]:
                entry_xml = '<entry>' + entry_xml.split('</entry>')[0] + '</entry>'
                try:
                    paper = parse_arxiv_entry(entry_xml)
                    if paper['pub_dt'] >= since_dt and paper['id'] not in seen_ids:
                        seen_ids.add(paper['id'])
                        collected.append(paper)
                        if len(collected) >= target_count:
                            break
                except Exception as e:
                    continue
        except Exception as e:
            print(f"âš ï¸ æŸ¥è¯¢å¤±è´¥: {q[:60]}... {e}")
            continue
    return collected

def generate_report(days_back=1):
    since_dt = datetime.now(timezone.utc) - timedelta(days=days_back)
    queries = generate_queries(KEYWORDS, max_combine=MAX_COMBINE)
    print(f"ğŸ“… æœç´¢è¿‡å» {days_back} å¤©")
    print(f"ğŸ” ç”Ÿæˆ {len(queries)} æ¡æŸ¥è¯¢è¯­å¥")
    all_papers = fetch_arxiv_papers(queries, since_dt, TARGET_COUNT * len(KEYWORDS))  # ç²—ç•¥è®¾æ€»æ•°

    if not all_papers:
        return "âŒ ä»Šæ—¥æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®"

    # æŒ‰IDå»é‡ï¼ˆå·²ç”±fetchå†…éƒ¨å»é‡ï¼‰
    lines = []
    lines.append(f"# ğŸ“š arXivæ¯æ—¥æ–‡çŒ®ç›‘æ§æŠ¥å‘Š")
    lines.append(f"**æŠ¥å‘Šæ—¥æœŸ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"**æœç´¢èŒƒå›´**: æœ€è¿‘ {days_back} å¤©")
    lines.append(f"**æ–°è®ºæ–‡æ•°**: {len(all_papers)} ç¯‡")
    lines.append("")

    for i, p in enumerate(all_papers, 1):
        authors = p['authors'][:2]
        author_str = ', '.join(authors) + ('ç­‰' if len(p['authors']) > 2 else '')
        categories = ', '.join(p['categories'][:2])
        lines.append(f"### {i}. {p['title']}")
        lines.append("")
        lines.append(f"**ID**: `{p['id']}`  ")
        lines.append(f"**ä½œè€…**: {author_str}  ")
        lines.append(f"**å‘å¸ƒæ—¶é—´**: {p['published']}  ")
        lines.append(f"**åˆ†ç±»**: {categories}  ")
        lines.append(f"**PDF**: [ä¸‹è½½é“¾æ¥]({p['link']})  ")
        lines.append(f"**arXiv**: [æŸ¥çœ‹é¡µé¢]({p['link']})  ")
        lines.append("")
        lines.append(f"**æ‘˜è¦**:")
        summary = p['summary'].replace('\n', ' ')
        lines.append(f"> {summary}")
        lines.append("")

    lines.append("---")
    lines.append("*è‡ªåŠ¨ç”Ÿæˆäº arXiv ç›‘æ§ç³»ç»Ÿ*")
    return "\n".join(lines)

def save_report(content):
    date_str = datetime.now().strftime("%Y%m%d")
    filename = f"arxiv_daily_report_{date_str}.md"
    filepath = OUTPUT_DIR / filename
    filepath.write_text(content, encoding='utf-8')
    return filepath

def send_to_feishu(text, webhook_url, secret=None):
    lines = text.split('\n')
    summary = '\n'.join(lines[:50])
    if len(lines) > 50:
        summary += "\n\n... (æŠ¥å‘Šè¿‡é•¿ï¼Œè¯·æŸ¥çœ‹å®Œæ•´æ–‡ä»¶)"
    payload = {"msg_type": "text", "content": {"text": summary}}
    if secret:
        timestamp = str(int(time.time()))
        string_to_sign = timestamp + "\n" + secret
        sign = base64.b64encode(hmac.new(string_to_sign.encode('utf-8'), digestmod=hashlib.sha256).digest()).decode('utf-8')
        payload["timestamp"] = timestamp
        payload["sign"] = sign
    try:
        resp = requests.post(webhook_url, json=payload, timeout=10)
        if resp.status_code == 200:
            result = resp.json()
            if result.get("code") == 0:
                print("âœ… å·²å‘é€åˆ°é£ä¹¦")
            else:
                print(f"âŒ é£ä¹¦è¿”å›é”™è¯¯: {result}")
        else:
            print(f"âŒ å‘é€å¤±è´¥ HTTP {resp.status_code}")
    except Exception as e:
        print(f"âŒ å‘é€å¼‚å¸¸: {e}")

def main():
    if hasattr(sys.stdout, 'buffer'):
        sys.stdout = open(sys.stdout.fileno(), 'w', encoding='utf-8', buffering=1)
        sys.stderr = open(sys.stderr.fileno(), 'w', encoding='utf-8', buffering=1)

    parser = argparse.ArgumentParser(description='arXivæ¯æ—¥æ–‡çŒ®æŠ¥å‘Š')
    parser.add_argument('--days', type=int, default=1, help='æœç´¢è¿‡å»å¤šå°‘å¤©çš„æ–‡çŒ® (é»˜è®¤: 1)')
    parser.add_argument('--output', choices=['markdown', 'text'], default='markdown', help='è¾“å‡ºæ ¼å¼')
    parser.add_argument('--save', action='store_true', help='ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶')
    args = parser.parse_args()

    report = generate_report(days_back=args.days)

    if args.output == 'text':
        text_report = re.sub(r'#+\s*', '', report)
        text_report = re.sub(r'\*\*(.*?)\*\*', r'\1', text_report)
        text_report = re.sub(r'\[(.*?)\]\(.*?\)', r'\1', text_report)
        print(text_report)
    else:
        print(report)

    if args.save:
        filepath = save_report(report)
        print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filepath}")

    webhook = os.getenv("FEISHU_WEBHOOK_URL")
    secret = os.getenv("FEISHU_SECRET")
    if webhook:
        send_to_feishu(report, webhook, secret)
    else:
        print("âš ï¸ æœªè®¾ç½® FEISHU_WEBHOOK_URLï¼Œè·³è¿‡å‘é€")

if __name__ == "__main__":
    main()
