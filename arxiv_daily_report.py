#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæºè®ºæ–‡ç›‘æ§ç³»ç»Ÿï¼šarXiv + IOP Science (via nsearch)
âœ… arXiv é¢„å°æœ¬ï¼ˆæ¯æ—¥æ›´æ–°ï¼‰
âœ… IOP æ­£å¼è®ºæ–‡ï¼ˆJapanese Journal of Applied Physics ç­‰ï¼‰
âœ… æ”¯æŒ QSL / é˜»æŒ«ç£ä½“ / ç£ç”µè€¦åˆ / å•æ™¶ç”Ÿé•¿
âœ… DeepSeek ä¸­æ–‡æ‘˜è¦ + é£ä¹¦æ¨é€ï¼ˆæ”¯æŒç­¾åï¼‰
"""

import os
import sys
import requests
import json
import time
import hashlib
import base64
import hmac
from pathlib import Path
from datetime import datetime, timedelta, timezone
from urllib.parse import quote_plus
from bs4 import BeautifulSoup
import re

# ==================== é…ç½®åŒº ====================
# ä»ç¯å¢ƒå˜é‡è¯»å–ï¼ˆGitHub Secrets æ³¨å…¥ï¼‰
FEISHU_WEBHOOK_URL = os.getenv("FEISHU_WEBHOOK_URL")
FEISHU_SECRET = os.getenv("FEISHU_SECRET")          # å¯é€‰ï¼Œå¦‚æœå¼€å¯äº†ç­¾ååˆ™å¿…é¡»æä¾›
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

if not FEISHU_WEBHOOK_URL:
    print("âŒ æœªè®¾ç½® FEISHU_WEBHOOK_URLï¼Œæ— æ³•å‘é€é£ä¹¦æ¶ˆæ¯")
    sys.exit(1)
if not DEEPSEEK_API_KEY:
    print("âŒ æœªè®¾ç½® DEEPSEEK_API_KEYï¼Œæ— æ³•ç¿»è¯‘æ‘˜è¦")
    sys.exit(1)

# === arXiv æ£€ç´¢ç­–ç•¥ï¼ˆä½¿ç”¨ abs: å­—æ®µï¼Œé¿å…è¯­æ³•é”™è¯¯ï¼‰===
ARXIV_TOPICS = [
    {
        "name": "ã€arXiv-QSLã€‘",
        "queries": [
            'abs:"quantum spin liquid"',
            'abs:"quantum spin liquid" abs:pyrochlore',
            'abs:QSL abs:"frustrated magnet"',
            'abs:"spin liquid" abs:"geometric frustration"',
            'abs:"kagome" abs:"geometric frustration"'
        ],
        "target_count": 5
    },
    {
        "name": "ã€arXiv-ç”Ÿé•¿/ç£ç”µã€‘",
        "queries": [
            'abs:"single crystal growth" abs:magnet',
            'abs:"flux growth" abs:"quantum magnet"',
            'abs:"magnetoelectric" abs:kagome',
            'abs:multiferroic abs:"frustrated"'
        ],
        "target_count": 3
    }
]

# === IOP nsearch æœç´¢è¯ ===
IOP_SEARCH_TERMS = [
    "quantum spin liquid frustrated magnet",
    "single crystal growth kagome pyrochlore",
    "magnetoelectric frustrated quantum magnet",
    "flux growth RuCl3 Herbertsmithite"
]

TIME_WINDOW_HOURS = 168  # æŸ¥æœ€è¿‘7å¤©
SENT_IDS_FILE = Path(__file__).parent / "sent_papers.json"

# ==================== å·¥å…·å‡½æ•° ====================
def load_sent_ids():
    if SENT_IDS_FILE.exists():
        try:
            return set(json.loads(SENT_IDS_FILE.read_text(encoding="utf-8")))
        except:
            return set()
    return set()

def save_sent_ids(ids):
    SENT_IDS_FILE.write_text(json.dumps(list(ids), indent=2), encoding="utf-8")

# --- arXiv ç›¸å…³ ---
def query_arxiv_raw(query_str, max_results=30, timeout=30):
    base_url = "https://export.arxiv.org/api/query"
    url = f"{base_url}?search_query={quote_plus(query_str)}&sortBy=submittedDate&sortOrder=descending&start=0&max_results={max_results}"
    response = requests.get(url, timeout=timeout)
    response.raise_for_status()
    return response.text

def parse_arxiv_xml(xml_text, since_dt):
    entries = []
    for entry in xml_text.split("<entry>")[1:]:
        try:
            title = entry.split("<title>")[1].split("</title>")[0].strip()
            summary = entry.split("<summary>")[1].split("</summary>")[0].strip()
            link = entry.split('<link href="')[1].split('"')[0]
            paper_id = "arxiv:" + link.split("/abs/")[-1]
            published = entry.split("<published>")[1].split("</published>")[0]
            pub_dt = datetime.strptime(published[:19], "%Y-%m-%dT%H:%M:%S").replace(tzinfo=timezone.utc)
            if pub_dt >= since_dt:
                entries.append({"id": paper_id, "title": title, "summary": summary, "link": link})
        except:
            continue
    return entries

# --- IOP nsearch æŠ“å– ---
def fetch_iop_nsearch_papers(keywords, since_dt):
    base_url = "https://iopscience.iop.org/nsearch"
    params = {"terms": keywords, "sort": "publishDate"}
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Accept-Encoding": "gzip, deflate",
        "Connection": "keep-alive",
    }
    try:
        response = requests.get(base_url, params=params, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        papers = []
        for item in soup.select('div.list-item'):
            try:
                title_tag = item.select_one('h3 a')
                if not title_tag:
                    continue
                title = title_tag.get_text(strip=True)
                link = "https://iopscience.iop.org" + title_tag['href']
                abs_tag = item.select_one('.abstract')
                abstract = abs_tag.get_text(strip=True) if abs_tag else ""
                date_tag = item.select_one('.pub-date')
                if not date_tag:
                    continue
                date_str = date_tag.get_text()
                match = re.search(r'(\d{1,2})\s+(\w+)\s+(\d{4})', date_str)
                if not match:
                    continue
                day, month, year = match.groups()
                pub_date = datetime.strptime(f"{day} {month} {year}", "%d %b %Y").replace(tzinfo=timezone.utc)
                if pub_date >= since_dt:
                    paper_id = f"iop:{link.split('/')[-1]}"
                    papers.append({
                        "id": paper_id,
                        "title": title,
                        "summary": abstract,
                        "link": link
                    })
            except Exception:
                continue
        return papers
    except Exception as e:
        print(f"âš ï¸ IOP nsearch æŠ“å–å¤±è´¥ ({keywords}): {e}")
        return []

# --- æ‘˜è¦ç¿»è¯‘ ---
def summarize_with_deepseek(text):
    if not text.strip():
        return "ã€æ‘˜è¦ã€‘æ— æ‘˜è¦ã€‚"
    prompt = (
        "ä½ æ˜¯ä¸€ä½é¡¶å°–å‡èšæ€ç‰©ç†å­¦å®¶ã€‚è¯·å°†ä»¥ä¸‹è‹±æ–‡è®ºæ–‡æ‘˜è¦ç¿»è¯‘æˆä¸“ä¸šã€ç®€æ´çš„ä¸­æ–‡ï¼Œå¹¶æç‚¼å‡ºæ ¸å¿ƒåˆ›æ–°ç‚¹ï¼ˆ100å­—ä»¥å†…ï¼‰ã€‚"
        f"\n\n{text}\n\n"
        "è¾“å‡ºæ ¼å¼ï¼šã€ä¸­æ–‡æ‘˜è¦ã€‘... ã€æ ¸å¿ƒåˆ›æ–°ã€‘..."
    )
    headers = {"Authorization": f"Bearer {DEEPSEEK_API_KEY}", "Content-Type": "application/json"}
    data = {"model": "deepseek-coder", "messages": [{"role": "user", "content": prompt}], "max_tokens": 300}
    try:
        resp = requests.post("https://api.deepseek.com/chat/completions", headers=headers, json=data, timeout=20)
        if resp.status_code == 200:
            return resp.json()["choices"][0]["message"]["content"].strip()
        else:
            return f"ã€æ‘˜è¦ã€‘{text[:200]}..."
    except:
        return f"ã€æ‘˜è¦ã€‘{text[:200]}..."

# --- é£ä¹¦æ¨é€ï¼ˆæ”¯æŒç­¾åï¼‰---
def send_to_feishu(title, deep_summary, link, tag):
    # æ„é€ é£ä¹¦å¯Œæ–‡æœ¬æ¶ˆæ¯
    content = {
        "msg_type": "post",
        "content": {
            "post": {
                "zh_cn": {
                    "title": f"{tag} {title}",
                    "content": [
                        [{"tag": "text", "text": deep_summary}],
                        [{"tag": "a", "text": "æŸ¥çœ‹å…¨æ–‡", "href": link}]
                    ]
                }
            }
        }
    }
    # å¦‚æœè®¾ç½®äº†ç­¾åå¯†é’¥ï¼Œæ·»åŠ ç­¾å
    if FEISHU_SECRET:
        timestamp = str(int(time.time()))
        string_to_sign = timestamp + "\n" + FEISHU_SECRET
        sign = base64.b64encode(
            hmac.new(string_to_sign.encode('utf-8'), digestmod=hashlib.sha256).digest()
        ).decode('utf-8')
        content["timestamp"] = timestamp
        content["sign"] = sign
    try:
        resp = requests.post(FEISHU_WEBHOOK_URL, json=content, timeout=10)
        if resp.status_code == 200:
            result = resp.json()
            if result.get("code") == 0:
                print(f"âœ… å·²å‘é€åˆ°é£ä¹¦: {title[:30]}...")
            else:
                print(f"âŒ é£ä¹¦è¿”å›é”™è¯¯: {result}")
        else:
            print(f"âŒ å‘é€å¤±è´¥ HTTP {resp.status_code}")
    except Exception as e:
        print(f"âŒ å‘é€å¼‚å¸¸: {e}")

# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ å¯åŠ¨å¤šæºè®ºæ–‡ç›‘æ§ç³»ç»Ÿ")
    print("ğŸ“š æ¥æºï¼šarXiv + IOP Science (nsearch)")
    print("=" * 60)
    
    sent_ids = load_sent_ids()
    all_new_papers = []
    since_dt = datetime.now(timezone.utc) - timedelta(hours=TIME_WINDOW_HOURS)
    
    # 1. æŠ“å– arXiv
    for topic in ARXIV_TOPICS:
        print(f"\nğŸ” æ£€ç´¢ arXiv: {topic['name']}")
        collected = 0
        for q in topic["queries"]:
            if collected >= topic["target_count"]:
                break
            try:
                xml = query_arxiv_raw(q, max_results=25)
                papers = parse_arxiv_xml(xml, since_dt)
                for p in papers:
                    if p["id"] not in sent_ids and p["id"] not in [x["id"] for x in all_new_papers]:
                        print(f"  ğŸ§  arXiv: {p['title'][:50]}...")
                        p["deep_summary"] = summarize_with_deepseek(p["summary"])
                        p["tag"] = topic["name"]
                        all_new_papers.append(p)
                        sent_ids.add(p["id"])
                        collected += 1
                        if collected >= topic["target_count"]:
                            break
            except Exception as e:
                print(f"  âš ï¸ arXiv æŸ¥è¯¢å¤±è´¥: {e}")
        print(f"  âœ… è·å– {collected} ç¯‡")

    # 2. æŠ“å– IOP
    print("\nğŸ“¡ æœç´¢ IOP Science (nsearch) ...")
    iop_count = 0
    for terms in IOP_SEARCH_TERMS:
        iop_papers = fetch_iop_nsearch_papers(terms, since_dt)
        for p in iop_papers:
            if p["id"] not in sent_ids:
                print(f"  ğŸ§  IOP: {p['title'][:50]}...")
                p["deep_summary"] = summarize_with_deepseek(p["summary"])
                p["tag"] = "ã€IOPã€‘"
                all_new_papers.append(p)
                sent_ids.add(p["id"])
                iop_count += 1
    print(f"  âœ… IOP å…±è·å– {iop_count} ç¯‡")
    
    total = len(all_new_papers)
    print(f"\nğŸ“¬ æ€»å…±æ‰¾åˆ°æ–°è®ºæ–‡: {total} ç¯‡")
    
    # 3. æ¨é€
    for p in all_new_papers:
        send_to_feishu(p["title"], p["deep_summary"], p["link"], p["tag"])
    
    save_sent_ids(sent_ids)
    print(f"\nâœ… ä»»åŠ¡å®Œæˆï¼å·²è®°å½• {len(sent_ids)} ç¯‡è®ºæ–‡ã€‚")
