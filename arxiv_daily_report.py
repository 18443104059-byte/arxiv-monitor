#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæºè®ºæ–‡ç›‘æ§ç³»ç»Ÿï¼ˆå¢å¼ºç‰ˆï¼‰
âœ… åŠ¨æ€æ‰©å¤§æœç´¢æ—¶é—´çª—å£ï¼Œç¡®ä¿æ¯æ—¥æœ‰æ¨é€
âœ… ä¸‰å¤§ä¸»é¢˜ + åˆ¶å¤‡æ–¹æ³•ç»„åˆæŸ¥è¯¢
âœ… DeepSeek ç¿»è¯‘ + é£ä¹¦ç­¾åæ¨é€
"""

import os
import sys
import requests
import json
import time
import hashlib
import base64
import hmac
from pathlib import Path
from datetime import datetime, timedelta, timezone
from urllib.parse import quote_plus
from bs4 import BeautifulSoup
import re

# ==================== ç¯å¢ƒå˜é‡é…ç½® ====================
FEISHU_WEBHOOK_URL = os.getenv("FEISHU_WEBHOOK_URL")
FEISHU_SECRET = os.getenv("FEISHU_SECRET")
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

if not FEISHU_WEBHOOK_URL:
    print("âŒ é”™è¯¯ï¼šæœªè®¾ç½®ç¯å¢ƒå˜é‡ FEISHU_WEBHOOK_URL")
    sys.exit(1)

# ==================== æœç´¢é…ç½® ====================
# ä¸‰å¤§ä¸»é¢˜åŠå…¶æŸ¥è¯¢ï¼ˆåŒ…å«åˆ¶å¤‡æ–¹æ³•ï¼‰
ARXIV_TOPICS = [
    {
        "name": "ã€å¤šé“/ç£ç”µ + åˆ¶å¤‡ã€‘",
        "queries": [
            'abs:"multiferroic"',
            'abs:"magnetoelectric"',
            'abs:"multiferroic" abs:"solid state reaction"',
            'abs:"multiferroic" abs:sintering',
            'abs:"multiferroic" abs:"ceramic method"',
            'abs:"multiferroic" abs:"chemical vapor transport"',
            'abs:"multiferroic" abs:"CVT"',
            'abs:"magnetoelectric" abs:"solid state reaction"',
            'abs:"magnetoelectric" abs:sintering',
            'abs:"magnetoelectric" abs:"ceramic method"',
            'abs:"magnetoelectric" abs:"chemical vapor transport"',
            'abs:"magnetoelectric" abs:"CVT"',
        ],
        "target_count": 5
    },
    {
        "name": "ã€é‡å­è‡ªæ—‹æ¶²ä½“ + åˆ¶å¤‡ã€‘",
        "queries": [
            'abs:"quantum spin liquid"',
            'abs:"QSL" abs:"frustrated magnet"',
            'abs:"spin liquid" abs:"geometric frustration"',
            'abs:"quantum spin liquid" abs:"solid state reaction"',
            'abs:"quantum spin liquid" abs:sintering',
            'abs:"quantum spin liquid" abs:"chemical vapor transport"',
            'abs:"quantum spin liquid" abs:"CVT"',
            'abs:"frustrated magnet" abs:"solid state reaction"',
            'abs:"frustrated magnet" abs:"single crystal growth"',
        ],
        "target_count": 5
    },
    {
        "name": "ã€Kagome + åˆ¶å¤‡ã€‘",
        "queries": [
            'abs:"kagome"',
            'abs:"kagome lattice"',
            'abs:"kagome" abs:"solid state reaction"',
            'abs:"kagome" abs:sintering',
            'abs:"kagome" abs:"chemical vapor transport"',
            'abs:"kagome" abs:"CVT"',
            'abs:"kagome" abs:"single crystal"',
        ],
        "target_count": 4
    },
    {
        "name": "ã€åˆ¶å¤‡æ–¹æ³•ä¸“é¢˜ã€‘",
        "queries": [
            'abs:"solid state reaction" abs:"multiferroic"',
            'abs:"solid state reaction" abs:"quantum spin liquid"',
            'abs:"solid state reaction" abs:"kagome"',
            'abs:"chemical vapor transport" abs:"multiferroic"',
            'abs:"chemical vapor transport" abs:"quantum spin liquid"',
            'abs:"chemical vapor transport" abs:"kagome"',
            'abs:"flux growth" abs:"frustrated magnet"',
        ],
        "target_count": 3
    }
]

# IOP æœç´¢è¯ï¼ˆåŒæ ·èå…¥åˆ¶å¤‡æ–¹æ³•ï¼‰
IOP_SEARCH_TERMS = [
    "multiferroic magnetoelectric solid state reaction",
    "multiferroic magnetoelectric ceramic method",
    "multiferroic magnetoelectric CVT",
    "quantum spin liquid frustrated magnet solid state",
    "quantum spin liquid frustrated magnet CVT",
    "kagome lattice solid state reaction",
    "kagome lattice sintering",
    "kagome lattice CVT",
    "solid state reaction multiferroic",
    "chemical vapor transport quantum spin liquid"
]

# åŠ¨æ€æ—¶é—´çª—å£é…ç½®ï¼ˆå•ä½ï¼šå¤©ï¼‰
TIME_WINDOWS = [7, 14, 30, 90]  # ä¾æ¬¡æ‰©å¤§
SENT_IDS_FILE = Path(__file__).parent / "sent_papers.json"

# ==================== å·¥å…·å‡½æ•° ====================
def load_sent_ids():
    if SENT_IDS_FILE.exists():
        try:
            return set(json.loads(SENT_IDS_FILE.read_text(encoding="utf-8")))
        except:
            return set()
    return set()

def save_sent_ids(ids):
    SENT_IDS_FILE.write_text(json.dumps(list(ids), indent=2), encoding="utf-8")

# --- arXiv ç›¸å…³ ---
def query_arxiv_raw(query_str, max_results=30, timeout=30):
    base_url = "https://export.arxiv.org/api/query"
    url = f"{base_url}?search_query={quote_plus(query_str)}&sortBy=submittedDate&sortOrder=descending&start=0&max_results={max_results}"
    response = requests.get(url, timeout=timeout)
    response.raise_for_status()
    return response.text

def parse_arxiv_xml(xml_text, since_dt):
    entries = []
    for entry in xml_text.split("<entry>")[1:]:
        try:
            title = entry.split("<title>")[1].split("</title>")[0].strip()
            summary = entry.split("<summary>")[1].split("</summary>")[0].strip()
            link = entry.split('<link href="')[1].split('"')[0]
            paper_id = "arxiv:" + link.split("/abs/")[-1]
            published = entry.split("<published>")[1].split("</published>")[0]
            pub_dt = datetime.strptime(published[:19], "%Y-%m-%dT%H:%M:%S").replace(tzinfo=timezone.utc)
            if pub_dt >= since_dt:
                entries.append({"id": paper_id, "title": title, "summary": summary, "link": link})
        except:
            continue
    return entries

# --- IOP nsearch æŠ“å– ---
def fetch_iop_nsearch_papers(keywords, since_dt):
    base_url = "https://iopscience.iop.org/nsearch"
    params = {"terms": keywords, "sort": "publishDate"}
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Accept-Encoding": "gzip, deflate",
        "Connection": "keep-alive",
    }
    try:
        response = requests.get(base_url, params=params, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        papers = []
        for item in soup.select('div.list-item'):
            try:
                title_tag = item.select_one('h3 a')
                if not title_tag:
                    continue
                title = title_tag.get_text(strip=True)
                link = "https://iopscience.iop.org" + title_tag['href']
                abs_tag = item.select_one('.abstract')
                abstract = abs_tag.get_text(strip=True) if abs_tag else ""
                date_tag = item.select_one('.pub-date')
                if not date_tag:
                    continue
                date_str = date_tag.get_text()
                match = re.search(r'(\d{1,2})\s+(\w+)\s+(\d{4})', date_str)
                if not match:
                    continue
                day, month, year = match.groups()
                pub_date = datetime.strptime(f"{day} {month} {year}", "%d %b %Y").replace(tzinfo=timezone.utc)
                if pub_date >= since_dt:
                    paper_id = f"iop:{link.split('/')[-1]}"
                    papers.append({
                        "id": paper_id,
                        "title": title,
                        "summary": abstract,
                        "link": link
                    })
            except Exception:
                continue
        return papers
    except Exception as e:
        print(f"âš ï¸ IOP nsearch æŠ“å–å¤±è´¥ ({keywords}): {e}")
        return []

# --- DeepSeek æ‘˜è¦ç¿»è¯‘ ---
def summarize_with_deepseek(text):
    if not text.strip():
        return "ã€æ‘˜è¦ã€‘æ— æ‘˜è¦ã€‚"
    if DEEPSEEK_API_KEY:
        prompt = (
            "ä½ æ˜¯ä¸€ä½é¡¶å°–å‡èšæ€ç‰©ç†å­¦å®¶ã€‚è¯·å°†ä»¥ä¸‹è‹±æ–‡è®ºæ–‡æ‘˜è¦ç¿»è¯‘æˆä¸“ä¸šã€ç®€æ´çš„ä¸­æ–‡ï¼Œå¹¶æç‚¼å‡ºæ ¸å¿ƒåˆ›æ–°ç‚¹ï¼ˆ100å­—ä»¥å†…ï¼‰ã€‚"
            f"\n\n{text}\n\n"
            "è¾“å‡ºæ ¼å¼ï¼šã€ä¸­æ–‡æ‘˜è¦ã€‘... ã€æ ¸å¿ƒåˆ›æ–°ã€‘..."
        )
        headers = {"Authorization": f"Bearer {DEEPSEEK_API_KEY}", "Content-Type": "application/json"}
        data = {"model": "deepseek-coder", "messages": [{"role": "user", "content": prompt}], "max_tokens": 300}
        try:
            resp = requests.post("https://api.deepseek.com/chat/completions", headers=headers, json=data, timeout=20)
            if resp.status_code == 200:
                return resp.json()["choices"][0]["message"]["content"].strip()
            else:
                print(f"âš ï¸ DeepSeek API è¿”å›é”™è¯¯ {resp.status_code}ï¼Œä½¿ç”¨åŸæ–‡æ‘˜è¦")
                return f"ã€æ‘˜è¦ã€‘{text[:200]}..."
        except Exception as e:
            print(f"âš ï¸ DeepSeek è°ƒç”¨å¼‚å¸¸: {e}ï¼Œä½¿ç”¨åŸæ–‡æ‘˜è¦")
            return f"ã€æ‘˜è¦ã€‘{text[:200]}..."
    else:
        return f"ã€æ‘˜è¦ã€‘{text[:200]}..."

# --- é£ä¹¦æ¨é€ï¼ˆæ”¯æŒç­¾åï¼‰---
def send_to_feishu(title, summary, link, tag):
    content = {
        "msg_type": "post",
        "content": {
            "post": {
                "zh_cn": {
                    "title": f"{tag} {title}",
                    "content": [
                        [{"tag": "text", "text": summary}],
                        [{"tag": "a", "text": "æŸ¥çœ‹å…¨æ–‡", "href": link}]
                    ]
                }
            }
        }
    }
    if FEISHU_SECRET:
        timestamp = str(int(time.time()))
        string_to_sign = timestamp + "\n" + FEISHU_SECRET
        sign = base64.b64encode(
            hmac.new(string_to_sign.encode('utf-8'), digestmod=hashlib.sha256).digest()
        ).decode('utf-8')
        content["timestamp"] = timestamp
        content["sign"] = sign
    try:
        resp = requests.post(FEISHU_WEBHOOK_URL, json=content, timeout=10)
        if resp.status_code == 200:
            result = resp.json()
            if result.get("code") == 0:
                print(f"âœ… å·²å‘é€åˆ°é£ä¹¦: {title[:30]}...")
            else:
                print(f"âŒ é£ä¹¦è¿”å›é”™è¯¯: {result}")
        else:
            print(f"âŒ å‘é€å¤±è´¥ HTTP {resp.status_code}")
    except Exception as e:
        print(f"âŒ å‘é€å¼‚å¸¸: {e}")

# ==================== åŠ¨æ€æ—¶é—´çª—å£æœç´¢ ====================
def search_papers_with_expanding_window():
    sent_ids = load_sent_ids()
    all_new_papers = []
    used_window = None

    for days in TIME_WINDOWS:
        since_dt = datetime.now(timezone.utc) - timedelta(days=days)
        print(f"\nğŸ“… å°è¯•æœç´¢æœ€è¿‘ {days} å¤©...")

        # ä¸´æ—¶å­˜å‚¨æœ¬æ¬¡çª—å£æ‰¾åˆ°çš„è®ºæ–‡ï¼ˆç”¨äºå»é‡ï¼‰
        window_papers = []

        # 1. æŠ“å– arXiv
        for topic in ARXIV_TOPICS:
            print(f"  ğŸ” æ£€ç´¢ arXiv: {topic['name']}")
            collected = 0
            for q in topic["queries"]:
                if collected >= topic["target_count"]:
                    break
                try:
                    xml = query_arxiv_raw(q, max_results=25)
                    papers = parse_arxiv_xml(xml, since_dt)
                    for p in papers:
                        if p["id"] not in sent_ids and p["id"] not in [x["id"] for x in window_papers]:
                            print(f"    ğŸ§  arXiv: {p['title'][:50]}...")
                            p["processed_summary"] = summarize_with_deepseek(p["summary"])
                            p["tag"] = topic["name"]
                            window_papers.append(p)
                            sent_ids.add(p["id"])
                            collected += 1
                            if collected >= topic["target_count"]:
                                break
                except Exception as e:
                    print(f"    âš ï¸ æŸ¥è¯¢å¤±è´¥: {e}")
                    continue

        # 2. æŠ“å– IOP
        print("  ğŸ“¡ æœç´¢ IOP Science (nsearch) ...")
        for terms in IOP_SEARCH_TERMS:
            iop_papers = fetch_iop_nsearch_papers(terms, since_dt)
            for p in iop_papers:
                if p["id"] not in sent_ids and p["id"] not in [x["id"] for x in window_papers]:
                    print(f"    ğŸ§  IOP: {p['title'][:50]}...")
                    p["processed_summary"] = summarize_with_deepseek(p["summary"])
                    p["tag"] = "ã€IOPã€‘"
                    window_papers.append(p)
                    sent_ids.add(p["id"])

        if window_papers:
            print(f"  âœ… åœ¨ {days} å¤©å†…æ‰¾åˆ° {len(window_papers)} ç¯‡æ–°è®ºæ–‡")
            all_new_papers = window_papers
            used_window = days
            break
        else:
            print(f"  âš ï¸ æœ€è¿‘ {days} å¤©æ— æ–°è®ºæ–‡ï¼Œæ‰©å¤§æ—¶é—´çª—å£...")

    return all_new_papers, used_window, sent_ids

# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ å¯åŠ¨å¤šæºè®ºæ–‡ç›‘æ§ç³»ç»Ÿï¼ˆå¢å¼ºç‰ˆï¼‰")
    print("ğŸ“š æ¥æºï¼šarXiv + IOP Science (nsearch)")
    print("=" * 60)

    new_papers, used_days, updated_sent_ids = search_papers_with_expanding_window()

    if not new_papers:
        print("\nâŒ æ‰€æœ‰æ—¶é—´çª—å£å‡æœªæ‰¾åˆ°æ–°è®ºæ–‡ã€‚")
        # å¯é€‰ï¼šå‘é€ä¸€æ¡æç¤ºæ¶ˆæ¯åˆ°é£ä¹¦
        msg = "ä»Šæ—¥ arXiv & IOP æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–°è®ºæ–‡ã€‚"
        send_to_feishu("ç³»ç»Ÿé€šçŸ¥", msg, "#", "ã€æç¤ºã€‘")
    else:
        print(f"\nğŸ“¬ å…±æ‰¾åˆ° {len(new_papers)} ç¯‡æ–°è®ºæ–‡ï¼ˆæ—¶é—´çª—å£ï¼šæœ€è¿‘ {used_days} å¤©ï¼‰")
        for p in new_papers:
            send_to_feishu(p["title"], p["processed_summary"], p["link"], p["tag"])

    save_sent_ids(updated_sent_ids)
    print(f"\nâœ… ä»»åŠ¡å®Œæˆï¼å·²è®°å½•è®ºæ–‡æ€»æ•°ï¼š{len(updated_sent_ids)} ç¯‡ã€‚")
