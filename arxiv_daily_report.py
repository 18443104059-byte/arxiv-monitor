#!/usr/bin/env python3
"""
arXivæ¯æ—¥æ–‡çŒ®æŠ¥å‘Šç”Ÿæˆ
"""

import argparse
import json
from datetime import datetime, timedelta
import os
import sys

def setup_encoding():
    """è®¾ç½®ç¼–ç ä»¥æ”¯æŒä¸­æ–‡"""
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = os.path.join(os.path.dirname(__file__), '..', 'config.yaml')
    
    # é»˜è®¤é…ç½®
    default_config = {
        'keywords': [
            'magnetoelectric coupling',
            'quantum spin liquid',
            'multiferroic',
            'topological insulator',
            'skyrmion',
            'spintronics',
            'condensed matter physics'
        ],
        'categories': ['cond-mat', 'physics'],
        'max_results': 15,
        'output_dir': './reports',
        'timezone': 'Asia/Shanghai'
    }
    
    # å¦‚æœé…ç½®æ–‡ä»¶å­˜åœ¨åˆ™åŠ è½½
    if os.path.exists(config_path):
        try:
            import yaml
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = yaml.safe_load(f)
                default_config.update(user_config)
        except:
            pass
    
    return default_config

def generate_daily_report(config, days_back=1):
    """ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š"""
    from arxiv_search import search_arxiv, filter_by_keywords, format_output
    
    print(f"ğŸ“Š ç”ŸæˆarXivæ¯æ—¥æ–‡çŒ®æŠ¥å‘Š")
    print(f"ğŸ“… æ—¥æœŸ: {datetime.now().strftime('%Y-%m-%d')}")
    print(f"ğŸ”‘ å…³é”®è¯: {', '.join(config['keywords'][:5])}...")
    
    # æœç´¢æ–‡çŒ®
    papers = search_arxiv(
        keywords=config['keywords'],
        max_results=config['max_results'],
        days_back=days_back
    )
    
    if not papers:
        return "âŒ ä»Šæ—¥æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®"
    
    # å…³é”®è¯è¿‡æ»¤
    filtered_papers = filter_by_keywords(papers, config['keywords'])
    
    # æŒ‰å…³é”®è¯åˆ†ç±»
    categorized = {}
    for paper in filtered_papers:
        keyword = paper.get('matched_keyword', 'å…¶ä»–')
        if keyword not in categorized:
            categorized[keyword] = []
        categorized[keyword].append(paper)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = []
    report.append(f"# ğŸ“š arXivæ¯æ—¥æ–‡çŒ®ç›‘æ§æŠ¥å‘Š")
    report.append(f"**æŠ¥å‘Šæ—¥æœŸ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append(f"**æœç´¢èŒƒå›´**: æœ€è¿‘{days_back}å¤©")
    report.append(f"**æ‰¾åˆ°æ–‡çŒ®**: {len(filtered_papers)}ç¯‡ (å…±æœç´¢åˆ°{len(papers)}ç¯‡)")
    report.append("")
    
    # æŒ‰å…³é”®è¯åˆ†ç±»å±•ç¤º
    for keyword, papers_in_category in categorized.items():
        report.append(f"## ğŸ” {keyword} ({len(papers_in_category)}ç¯‡)")
        report.append("")
        
        for i, paper in enumerate(papers_in_category, 1):
            report.append(f"### {i}. {paper['title']}")
            report.append("")
            report.append(f"**ID**: `{paper['id']}`  ")
            report.append(f"**ä½œè€…**: {', '.join(paper['authors'][:2])}" + 
                         ("ç­‰" if len(paper['authors']) > 2 else ""))
            report.append(f"**å‘å¸ƒæ—¶é—´**: {paper['published']}  ")
            report.append(f"**åˆ†ç±»**: {', '.join(paper['categories'][:2])}  ")
            report.append(f"**PDF**: [ä¸‹è½½é“¾æ¥]({paper['pdf_url']})  ")
            report.append(f"**arXiv**: [æŸ¥çœ‹é¡µé¢]({paper['arxiv_url']})  ")
            report.append("")
            report.append(f"**æ‘˜è¦**:")
            report.append(f"> {paper['summary']}")
            report.append("")
    
    # ç»Ÿè®¡ä¿¡æ¯
    report.append("## ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
    report.append("")
    report.append(f"- **æ€»æ–‡çŒ®æ•°**: {len(filtered_papers)}ç¯‡")
    report.append(f"- **å…³é”®è¯åˆ†å¸ƒ**:")
    for keyword, papers_in_category in categorized.items():
        report.append(f"  - {keyword}: {len(papers_in_category)}ç¯‡")
    
    report.append(f"- **æ—¶é—´èŒƒå›´**: {datetime.now().strftime('%Y-%m-%d')}")
    if days_back > 1:
        start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')
        report.append(f"- **æœç´¢åŒºé—´**: {start_date} è‡³ {datetime.now().strftime('%Y-%m-%d')}")
    
    report.append("")
    report.append("---")
    report.append("*è‡ªåŠ¨ç”Ÿæˆäº OpenClaw arXivç›‘æ§ç³»ç»Ÿ*")
    
    return '\n'.join(report)

def save_report(report, output_format='markdown'):
    """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = './reports'
    os.makedirs(output_dir, exist_ok=True)
    
    # ç”Ÿæˆæ–‡ä»¶å
    date_str = datetime.now().strftime("%Y%m%d")
    filename = f"arxiv_daily_report_{date_str}.md"
    filepath = os.path.join(output_dir, filename)
    
    # ä¿å­˜æ–‡ä»¶
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(report)
    
    return filepath

def send_to_feishu(report, webhook_url, secret=None):
    """
    å‘é€æŠ¥å‘Šåˆ°é£ä¹¦ï¼ˆæ”¯æŒç­¾åéªŒè¯ï¼‰
    :param report: æŠ¥å‘Šå†…å®¹ï¼ˆå­—ç¬¦ä¸²ï¼‰
    :param webhook_url: é£ä¹¦æœºå™¨äºº webhook åœ°å€
    :param secret: é£ä¹¦ç­¾åå¯†é’¥ï¼ˆå¦‚æœå¼€å¯äº†ç­¾åéªŒè¯ï¼‰
    """
    try:
        import requests
    except ImportError:
        print("âŒ æœªå®‰è£… requests åº“ï¼Œæ— æ³•å‘é€é£ä¹¦æ¶ˆæ¯")
        return

    # ç®€åŒ–æŠ¥å‘Šå†…å®¹ï¼ˆé£ä¹¦æ¶ˆæ¯æœ‰é•¿åº¦é™åˆ¶ï¼‰
    lines = report.split('\n')
    summary = '\n'.join(lines[:50])  # å–å‰ 50 è¡Œ
    if len(lines) > 50:
        summary += "\n\n... (æŠ¥å‘Šè¿‡é•¿ï¼Œè¯·æŸ¥çœ‹å®Œæ•´æ–‡ä»¶)"

    payload = {
        "msg_type": "text",
        "content": {
            "text": summary
        }
    }

    # å¦‚æœæä¾›äº† secretï¼Œåˆ™æ·»åŠ ç­¾å
    if secret:
        import hashlib
        import base64
        import hmac
        import time
        timestamp = str(int(time.time()))
        string_to_sign = timestamp + "\n" + secret
        sign = base64.b64encode(
            hmac.new(string_to_sign.encode('utf-8'), digestmod=hashlib.sha256).digest()
        ).decode('utf-8')
        payload["timestamp"] = timestamp
        payload["sign"] = sign

    try:
        response = requests.post(webhook_url, json=payload, timeout=10)
        if response.status_code == 200:
            result = response.json()
            if result.get("code") == 0:
                print("âœ… å·²å‘é€åˆ°é£ä¹¦")
            else:
                print(f"âŒ é£ä¹¦è¿”å›é”™è¯¯: {result}")
        else:
            print(f"âŒ å‘é€å¤±è´¥ HTTP {response.status_code}: {response.text}")
    except Exception as e:
        print(f"âŒ å‘é€å¼‚å¸¸: {e}")

def main():
    """ä¸»å‡½æ•°"""
    setup_encoding()
    
    parser = argparse.ArgumentParser(description='arXivæ¯æ—¥æ–‡çŒ®æŠ¥å‘Š')
    parser.add_argument('--days', type=int, default=1,
                       help='æœç´¢è¿‡å»å¤šå°‘å¤©çš„æ–‡çŒ® (é»˜è®¤: 1)')
    parser.add_argument('--output', choices=['markdown', 'text'],
                       default='markdown', help='è¾“å‡ºæ ¼å¼')
    parser.add_argument('--save', action='store_true',
                       help='ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    config = load_config()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generate_daily_report(config, args.days)
    
    # è¾“å‡ºæŠ¥å‘Š
    if args.output == 'text':
        # ç®€åŒ–æ–‡æœ¬æ ¼å¼
        import re
        text_report = re.sub(r'#+\s*', '', report)
        text_report = re.sub(r'\*\*(.*?)\*\*', r'\1', text_report)
        text_report = re.sub(r'\[(.*?)\]\(.*?\)', r'\1', text_report)
        print(text_report)
    else:
        print(report)
    
    # ä¿å­˜æŠ¥å‘Š
    if args.save:
        filepath = save_report(report)
        print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filepath}")

    # å‘é€åˆ°é£ä¹¦
    webhook_url = os.getenv("FEISHU_WEBHOOK_URL")
    secret = os.getenv("FEISHU_SECRET")   # è·å–ç­¾åå¯†é’¥
    if webhook_url:
        send_to_feishu(report, webhook_url, secret)
    else:
        print("âš ï¸ æœªè®¾ç½® FEISHU_WEBHOOK_URLï¼Œè·³è¿‡å‘é€")

if __name__ == "__main__":
    main()
