#!/usr/bin/env python3
"""
arXivæ–‡çŒ®æœç´¢è„šæœ¬
è‡ªåŠ¨æœç´¢ç£ç”µè€¦åˆå’Œé‡å­è‡ªæ—‹æ¶²ä½“ç›¸å…³æ–‡çŒ®
"""

import argparse
import requests
import feedparser
from datetime import datetime, timedelta
import json
import time
import sys
import os

def setup_encoding():
    """è®¾ç½®ç¼–ç ä»¥æ”¯æŒä¸­æ–‡"""
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

def search_arxiv(keywords, max_results=10, days_back=1):
    """
    æœç´¢arXivæ–‡çŒ®
    
    Args:
        keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
        max_results: æœ€å¤§è¿”å›ç»“æœæ•°
        days_back: æœç´¢è¿‡å»å¤šå°‘å¤©çš„æ–‡çŒ®
    
    Returns:
        æ–‡çŒ®åˆ—è¡¨
    """
    # arXiv API URL
    base_url = "http://export.arxiv.org/api/query"
    
    # æ„å»ºæœç´¢æŸ¥è¯¢
    query_parts = []
    for keyword in keywords:
        query_parts.append(f'all:"{keyword}"')
    
    query = " OR ".join(query_parts)
    
    # æ—¥æœŸè¿‡æ»¤
    if days_back > 0:
        start_date = (datetime.now() - timedelta(days=days_back)).strftime("%Y%m%d")
        query += f" AND submittedDate:[{start_date}000000 TO *]"
    
    # è¯·æ±‚å‚æ•°
    params = {
        'search_query': query,
        'start': 0,
        'max_results': max_results,
        'sortBy': 'submittedDate',
        'sortOrder': 'descending'
    }
    
    try:
        print(f"ğŸ” æœç´¢arXiv: {query}")
        print(f"ğŸ“… æ—¶é—´èŒƒå›´: æœ€è¿‘{days_back}å¤©")
        
        response = requests.get(base_url, params=params)
        response.raise_for_status()
        
        # è§£æAtom feed
        feed = feedparser.parse(response.content)
        
        papers = []
        for entry in feed.entries:
            paper = {
                'id': entry.id.split('/')[-1],
                'title': entry.title.replace('\n', ' ').strip(),
                'summary': entry.summary.replace('\n', ' ').strip()[:500] + "...",
                'authors': [author.name for author in entry.authors],
                'published': entry.published,
                'updated': entry.updated,
                'pdf_url': None,
                'arxiv_url': None,
                'categories': [tag.term for tag in entry.tags],
                'primary_category': entry.arxiv_primary_category['term'] if hasattr(entry, 'arxiv_primary_category') else None
            }
            
            # æŸ¥æ‰¾PDFé“¾æ¥
            for link in entry.links:
                if link.rel == 'alternate' and link.type == 'text/html':
                    paper['arxiv_url'] = link.href
                elif link.title == 'pdf':
                    paper['pdf_url'] = link.href
            
            papers.append(paper)
        
        return papers
    
    except Exception as e:
        print(f"âŒ æœç´¢å¤±è´¥: {e}")
        return []

def filter_by_keywords(papers, keywords):
    """æ ¹æ®å…³é”®è¯è¿‡æ»¤æ–‡çŒ®"""
    filtered = []
    for paper in papers:
        # æ£€æŸ¥æ ‡é¢˜å’Œæ‘˜è¦ä¸­æ˜¯å¦åŒ…å«å…³é”®è¯
        text = (paper['title'] + ' ' + paper['summary']).lower()
        for keyword in keywords:
            if keyword.lower() in text:
                paper['matched_keyword'] = keyword
                filtered.append(paper)
                break
    
    return filtered

def format_output(papers, output_format='text'):
    """æ ¼å¼åŒ–è¾“å‡º"""
    if output_format == 'json':
        return json.dumps(papers, ensure_ascii=False, indent=2)
    
    elif output_format == 'text':
        output = []
        output.append(f"ğŸ“š arXivæ–‡çŒ®æœç´¢ç»“æœ ({len(papers)}ç¯‡)")
        output.append("=" * 60)
        
        for i, paper in enumerate(papers, 1):
            output.append(f"\n{i}. {paper['title']}")
            output.append(f"   ğŸ“ ID: {paper['id']}")
            output.append(f"   ğŸ‘¥ ä½œè€…: {', '.join(paper['authors'][:3])}" + 
                         ("ç­‰" if len(paper['authors']) > 3 else ""))
            output.append(f"   ğŸ“… å‘å¸ƒæ—¶é—´: {paper['published']}")
            output.append(f"   ğŸ·ï¸ åˆ†ç±»: {', '.join(paper['categories'][:3])}")
            if 'matched_keyword' in paper:
                output.append(f"   ğŸ” åŒ¹é…å…³é”®è¯: {paper['matched_keyword']}")
            output.append(f"   ğŸ“„ PDF: {paper['pdf_url']}")
            output.append(f"   ğŸŒ arXiv: {paper['arxiv_url']}")
            output.append(f"   ğŸ“ æ‘˜è¦: {paper['summary'][:300]}...")
        
        return '\n'.join(output)
    
    elif output_format == 'markdown':
        output = []
        output.append(f"# ğŸ“š arXivæ–‡çŒ®ç›‘æ§æŠ¥å‘Š")
        output.append(f"**æœç´¢æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        output.append(f"**æ‰¾åˆ°æ–‡çŒ®**: {len(papers)}ç¯‡")
        output.append("")
        
        for i, paper in enumerate(papers, 1):
            output.append(f"## {i}. {paper['title']}")
            output.append("")
            output.append(f"**ID**: `{paper['id']}`  ")
            output.append(f"**ä½œè€…**: {', '.join(paper['authors'][:3])}" + 
                         ("ç­‰" if len(paper['authors']) > 3 else ""))
            output.append(f"**å‘å¸ƒæ—¶é—´**: {paper['published']}  ")
            output.append(f"**åˆ†ç±»**: {', '.join(paper['categories'][:3])}  ")
            if 'matched_keyword' in paper:
                output.append(f"**åŒ¹é…å…³é”®è¯**: `{paper['matched_keyword']}`  ")
            output.append(f"**PDF**: [ä¸‹è½½é“¾æ¥]({paper['pdf_url']})  ")
            output.append(f"**arXiv**: [æŸ¥çœ‹é¡µé¢]({paper['arxiv_url']})  ")
            output.append("")
            output.append(f"**æ‘˜è¦**:")
            output.append(f"> {paper['summary']}")
            output.append("")
            output.append("---")
            output.append("")
        
        return '\n'.join(output)

def main():
    """ä¸»å‡½æ•°"""
    setup_encoding()
    
    parser = argparse.ArgumentParser(description='arXivæ–‡çŒ®æœç´¢')
    parser.add_argument('--keywords', nargs='+', 
                       default=['magnetoelectric coupling', 'quantum spin liquid', 'multiferroic', 'topological'],
                       help='æœç´¢å…³é”®è¯')
    parser.add_argument('--max-results', type=int, default=10,
                       help='æœ€å¤§ç»“æœæ•° (é»˜è®¤: 10)')
    parser.add_argument('--days', type=int, default=1,
                       help='æœç´¢è¿‡å»å¤šå°‘å¤©çš„æ–‡çŒ® (é»˜è®¤: 1)')
    parser.add_argument('--output', choices=['json', 'text', 'markdown'],
                       default='text', help='è¾“å‡ºæ ¼å¼')
    parser.add_argument('--filter', action='store_true',
                       help='ä½¿ç”¨å…³é”®è¯è¿‡æ»¤ï¼ˆä¸¥æ ¼æ¨¡å¼ï¼‰')
    
    args = parser.parse_args()
    
    print(f"ğŸš€ å¼€å§‹æœç´¢arXivæ–‡çŒ®...")
    print(f"ğŸ”‘ å…³é”®è¯: {', '.join(args.keywords)}")
    
    # æœç´¢æ–‡çŒ®
    papers = search_arxiv(
        keywords=args.keywords,
        max_results=args.max_results,
        days_back=args.days
    )
    
    if not papers:
        print("âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡çŒ®")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(papers)} ç¯‡æ–‡çŒ®")
    
    # å…³é”®è¯è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
    if args.filter:
        filtered_papers = filter_by_keywords(papers, args.keywords)
        print(f"ğŸ” å…³é”®è¯è¿‡æ»¤å: {len(filtered_papers)} ç¯‡")
        papers = filtered_papers
    
    # è¾“å‡ºç»“æœ
    output = format_output(papers, args.output)
    print(output)
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"arxiv_results_{timestamp}.{args.output if args.output != 'markdown' else 'md'}"
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(output)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")

if __name__ == "__main__":
    main()